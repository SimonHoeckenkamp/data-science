{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 05 - Homework 05: Q-Value iteration\n",
    "\n",
    "\n",
    "\n",
    "Consider an Markov Decision Process with 6 states $s \\epsilon \\{0,1,2,3,4,5\\}$ and 2 actions $a \\epsilon \\{C,M\\}$, defined by the following transition probability functions\n",
    "For states 1, 2, and 3:\n",
    "\n",
    "$T(s,M,s−1)=1$\n",
    "\t \n",
    "$T(s,C,s+2)=0.7$\n",
    "\t \n",
    "$T(s,C,s)=0.3$\n",
    "\t \n",
    "\n",
    "For state 0:\n",
    "\n",
    "$T(s,M,s)=1$\n",
    "\t \n",
    "$T(s,C,s)=1$\n",
    "\t \n",
    "\n",
    "For states 4 and 5:\n",
    "\n",
    "$T(s,M,s−1)=1$\n",
    "\t \n",
    "$T(s,C,s)=1$\n",
    "\t \n",
    "\n",
    "Note that all transition probabilities not defined by the above are equal to 0.\n",
    "The rewards R are defined by:\n",
    "\n",
    "$R(s,a,s′)=|s′−s|^{1/3} ∀s≠s′$,\n",
    "\n",
    "and $R(s,a,s) = (s+4)^{−1/2}$, $∀s≠0$.\n",
    "\n",
    "$R(0,M,0) = R(0,C,0) = 0$. Also, the discount factor $\\gamma = 0.6$.\n",
    "\n",
    "We initialize $Q_0(s,a) = 0$ $∀ s \\epsilon \\{0,1,2,3,4,5\\}$ and $∀ a \\{C,M\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reward function\n",
    "def reward(state_1, state_2):\n",
    "    if state_1 == 0 and state_2 ==0:\n",
    "        return 0\n",
    "    elif state_1 != state_2:\n",
    "        return np.abs(state_2 - state_1) ** (1 / 3)\n",
    "    elif state_1 == state_2:\n",
    "        return (state_1 + 4) ** (-1 / 2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0.3, 0. , 0.7, 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0.3, 0. , 0.7, 0. ],\n",
       "        [0. , 1. , 0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0.3, 0. , 0.7],\n",
       "        [0. , 0. , 1. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. , 1. , 0. ],\n",
       "        [0. , 0. , 0. , 1. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. , 0. , 1. ],\n",
       "        [0. , 0. , 0. , 0. , 1. , 0. ]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define T-matrix (transition probabilities) 0: start-state, 1: action, 2: target-state\n",
    "# indices for actions: 0:\"C\", 1:\"M\"\n",
    "T = np.zeros((6,2,6))\n",
    "\n",
    "# set values for states 1,2,3:\n",
    "for i in [1,2,3]:\n",
    "    T[i,0,i+2] = 0.7\n",
    "    T[i,0,i] = 0.3\n",
    "    T[i,1,i-1] = 1\n",
    "\n",
    "# set values for state 0:\n",
    "T[0,0,0] = 1\n",
    "T[0,1,0] = 1\n",
    "\n",
    "#set values for states 4,5:\n",
    "for i in [4,5]:\n",
    "    T[i,1,i-1] = 1\n",
    "    T[i,0,i] = 1\n",
    "    \n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 1.        , 1.25992105, 1.44224957, 1.58740105,\n",
       "         1.70997595],\n",
       "        [0.        , 1.        , 1.25992105, 1.44224957, 1.58740105,\n",
       "         1.70997595]],\n",
       "\n",
       "       [[1.        , 0.4472136 , 1.        , 1.25992105, 1.44224957,\n",
       "         1.58740105],\n",
       "        [1.        , 0.4472136 , 1.        , 1.25992105, 1.44224957,\n",
       "         1.58740105]],\n",
       "\n",
       "       [[1.25992105, 1.        , 0.40824829, 1.        , 1.25992105,\n",
       "         1.44224957],\n",
       "        [1.25992105, 1.        , 0.40824829, 1.        , 1.25992105,\n",
       "         1.44224957]],\n",
       "\n",
       "       [[1.44224957, 1.25992105, 1.        , 0.37796447, 1.        ,\n",
       "         1.25992105],\n",
       "        [1.44224957, 1.25992105, 1.        , 0.37796447, 1.        ,\n",
       "         1.25992105]],\n",
       "\n",
       "       [[1.58740105, 1.44224957, 1.25992105, 1.        , 0.35355339,\n",
       "         1.        ],\n",
       "        [1.58740105, 1.44224957, 1.25992105, 1.        , 0.35355339,\n",
       "         1.        ]],\n",
       "\n",
       "       [[1.70997595, 1.58740105, 1.44224957, 1.25992105, 1.        ,\n",
       "         0.33333333],\n",
       "        [1.70997595, 1.58740105, 1.44224957, 1.25992105, 1.        ,\n",
       "         0.33333333]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define reward matrix\n",
    "R = np.zeros((6,2,6))\n",
    "\n",
    "for s_start in range(len(R)):\n",
    "    for a in range(R.shape[1]):\n",
    "        for s_target in range(R.shape[2]):\n",
    "            R[s_start, a, s_target] = reward(s_start, s_target)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gamma and initialization\n",
    "gamma = 0.6\n",
    "Q_0 = np.zeros((6, 2))\n",
    "V_0 = np.zeros(6,)"
   ]
  },
  {
   "source": [
    "Following code runs the Q-Value iteration algorithm and prints the Q-values after given no. of iterations. Individual states are represented by lines, column 0 represents the value of the \"C\"-action, column 1 represents the \"M\"-action.\n",
    "\n",
    "## TODO:\n",
    "- Convergence criteria can be implenented\n",
    "- Usage of matplotlib for better visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q ( iter  1 ): \n",
      " [[0.         0.        ]\n",
      " [1.01610881 1.        ]\n",
      " [1.00441922 1.        ]\n",
      " [0.99533408 1.        ]\n",
      " [0.35355339 1.        ]\n",
      " [0.33333333 1.        ]]\n",
      "V: \n",
      " [0.         1.01610881 1.00441922 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# run Q-step algorithm\n",
    "num_iter = 1\n",
    "Q = Q_0\n",
    "V = V_0\n",
    "for i in range(num_iter):\n",
    "    Q = np.sum(T * (R + gamma * np.max(Q, axis=1)), axis=2)\n",
    "    V = np.max(Q, axis=1)\n",
    "\n",
    "print(\"Q ( iter \", num_iter, \"): \\n\", Q)\n",
    "print(\"V: \\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}